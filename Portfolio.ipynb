{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adedbc2",
   "metadata": {
    "id": "1adedbc2"
   },
   "source": [
    "\n",
    "<div style=\"background-color:white; text-align:center; padding:20px;\">\n",
    "    <h2 style=\"color:black; font-family: Verdana, sans-serif;\"><strong>Warlord</strong></h2>\n",
    "    <p style=\"font-size: 14px; color: black; font-family: Verdana, sans-serif;\">\n",
    "        <table style=\"margin: auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 0;\">Names</th>\n",
    "                <th style=\"border: 0;\">GitHub Username</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Vince Ammerlaan</td>\n",
    "                <td style=\"border: 0;\">Vince16270</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Tommi Lander</td>\n",
    "                <td style=\"border: 0;\">tommi7</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Nima Ghafar</td>\n",
    "                <td style=\"border: 0;\">NimaGhafar</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Isa Dijkstra</td>\n",
    "                <td style=\"border: 0;\">IsaD01</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; margin-top: 10px;\">\n",
    "    <img src=\"photo/image1.png\" alt=\"\" style=\"width: 1000px; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa730d0d",
   "metadata": {
    "id": "fa730d0d"
   },
   "source": [
    "- Docent: Vikram Radhakrishnan\n",
    "- Datum: 22-06-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e751d",
   "metadata": {
    "id": "339e751d"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Inhoudsopgave </strong></h2>\n",
    "</div>\n",
    "<ul style=\"padding: 0; list-style: none;\">\n",
    "    <li style=\"text-align: center;\"><a href=\"#1.0\">Chapter 1: Het project</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#2.0\">Chapter 2: Packages en functies inladen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#3.0\">Chapter 3: Trainen van de agents</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#4.0\">Chapter 4: Resultaten van het trainen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 5: Conclusie</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 6: Bronnenlijst</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7f30",
   "metadata": {
    "id": "9ffa7f30"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 1: Het project </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c36653",
   "metadata": {
    "id": "25c36653"
   },
   "source": [
    "In deze opdracht richten we ons op het computerspel Warlords. In dit spel beschermd elke speler een kasteel dat zich in een van de vier hoeken van het scherm bevindt. Vanuit het midden van het scherm komt een bal die wordt teruggekaatst om de kastelen van de tegenstanders te vernietigen. Elke speler heeft een schild waarmee hij de bal kan afweren of richten op vijanden.\n",
    "\n",
    "Ons doel is om meerdere slimme agents te ontwikkelen die elk zelfstandig leren spelen via multi-agent reinforcement learning. Dat betekent dat elke agent leert op basis van zijn eigen ervaringen, maar ook rekening moet houden met het gedrag van andere agents. De omgeving is dynamisch en strategisch: je moet niet alleen goed verdedigen, maar ook slim aanvallen.\n",
    "\n",
    "Warlords is bij uitstek geschikt voor dit soort leerstrategieën, omdat het meerdere spelers bevat die tegelijk handelen en op elkaar reageren. Dat maakt het leerproces complex en interessant: agents moeten niet alleen leren van hun eigen fouten, maar ook anticiperen op anderen.\n",
    "\n",
    "Het spel eindigt op twee manieren:\n",
    "\n",
    "- Je wint door als laatste overgebleven speler in het spel te zijn.\n",
    "- Je verliest als je kasteel volledig vernietigd is door de bal.\n",
    "\n",
    "**Wat is het probleem?**\n",
    "\n",
    "We willen een slimme agent ontwikkelen die zelfstandig leert om het spel Warlords goed te spelen. Deze agent moet:\n",
    "\n",
    "- Leren hoe de bal beweegt en hoe hij daarop moet reageren\n",
    "- Ontwikkelen hoe je effectief verdedigt én aanvalt\n",
    "- Tactisch aanvallen uitvoeren om de vijandelijke kastelen te verzwakken\n",
    "- Opletten op meerdere tegenstanders tegelijk en zich aanpassen aan hun gedrag\n",
    "\n",
    "**Wat is het doel?**\n",
    "\n",
    "De agent moet:\n",
    "\n",
    "- Zelf leren van hun ervaringen en hun gedrag continu aanpassen\n",
    "- Beter presteren dan agents die willekeurig bewegen of vaste strategieën gebruiken\n",
    "- Verschillende strategieën ontwikkelen afhankelijk van tegenstanders\n",
    "- Leren overleven in een complexe omgeving en uiteindelijk winnen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b48b3",
   "metadata": {
    "id": "ed7b48b3"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 2: Packages en functies inladen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4583bc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4583bc96",
    "outputId": "abcce2be-109c-4f17-c436-cbb6826744b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc9c40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6dc9c40",
    "outputId": "522328ce-58a3-4d90-a7e4-0b7304c87e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.11/dist-packages (1.25.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.0.2)\n",
      "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (1.1.1)\n",
      "Requirement already satisfied: multi_agent_ale_py>=0.1.11 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (0.1.11)\n",
      "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (0.0.4)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.6.15)\n",
      "Looking in links: dist/\n",
      "Requirement already satisfied: AutoROM[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (8.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo[atari]\n",
    "!pip install \"autorom[accept-rom-license]\"\n",
    "!pip install --find-links dist/ --no-cache-dir AutoROM[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0hY7YjUvw3mx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hY7YjUvw3mx",
    "outputId": "1e429410-cf29-4970-f554-fe7ce393d37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoROM will download the Atari 2600 ROMs.\n",
      "They will be installed to:\n",
      "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
      "\t/usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms\n",
      "\n",
      "Existing ROMs will be overwritten.\n",
      "\n",
      "I own a license to these Atari 2600 ROMs.\n",
      "I agree to not distribute these ROMs and wish to proceed: [Y/n]: Y\n"
     ]
    }
   ],
   "source": [
    "# Start AutoROM\n",
    "\n",
    "!AutoROM\n",
    "\n",
    "# Import libraries\n",
    "from pettingzoo.atari import warlords_v3\n",
    "from pettingzoo.utils import BaseParallelWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import importlib\n",
    "import os\n",
    "import imageio\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38d3f8",
   "metadata": {
    "id": "1a38d3f8"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 3: Trainen van de agents </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca7b1-afbc-4ba0-a032-c2000b9e7371",
   "metadata": {
    "id": "ec9ca7b1-afbc-4ba0-a032-c2000b9e7371"
   },
   "source": [
    "**PPO Agent: Proximal Policy Optimization**\n",
    "\n",
    "Wij hebben ervoor gekozen om een PPO-agent te trainen. PPO staat voor Proximal Policy Optimization en is een state-of-the-art algoritme dat agents in staat stelt om zelfstandig te leren door middel van trial-and-error in een omgeving.\n",
    "\n",
    "De drie belangrijkste voordelen van PPO zijn:\n",
    "\n",
    "- Eenvoud\n",
    "- Stabiliteit\n",
    "- Steekproefefficiëntie\n",
    "\n",
    "\n",
    "Het PPO-algoritme werkt volgens de volgende principes:\n",
    "\n",
    "- *Policy Gradient Methods*:\n",
    "PPO leert direct welk gedrag (actie) optimaal is in een bepaalde situatie (state), in plaats van alleen in te schatten hoe goed een situatie op zichzelf is. In onze agent is dit terug te zien in hoe acties worden gekozen: op basis van een kansverdeling die voortkomt uit een getraind neuraal netwerk.\n",
    "- *Objective Function*:\n",
    "Het algoritme probeert het gedrag van de agent zo aan te passen dat deze op de lange termijn zoveel mogelijk beloning ontvangt uit de omgeving. Dit gebeurt aan de hand van een speciaal ontworpen loss functie die gebaseerd is op de verwachte toekomstige beloningen. Onze agent optimaliseert deze functie om het gedrag voortdurend te verbeteren.\n",
    "\n",
    "- *Clipped Surrogate Objective*:\n",
    "PPO voorkomt dat de agent na één leerstap drastisch ander gedrag gaat vertonen. Grote aanpassingen in het gedrag worden ‘geclipt’, waardoor het leerproces stabiel blijft. Dit principe is verwerkt in de manier waarop wij de verhouding tussen oude en nieuwe actiekeuzes beperken tijdens het optimaliseren.\n",
    "\n",
    "- *Multiple Epochs & Mini-Batch Updates*:\n",
    "De agent verzamelt ervaringen (observaties, acties, beloningen) in de omgeving. Deze gegevens worden vervolgens herhaaldelijk gebruikt tijdens het leerproces, opgesplitst in kleine, efficiënte mini-batches.\n",
    "\n",
    "- *Value Function Estimation*:\n",
    "PPO maakt ook gebruik van een waarde-inschatting van situaties (de ‘value function’), wat helpt om het leerproces minder schommelend en efficiënter te maken. In onze agent is dit geïntegreerd in het neurale netwerk dat zowel de actievoorspellingen als de waarde-inschattingen doet. Deze waarde-inschatting maakt het leren sneller en stabieler.\n",
    "\n",
    "- *Parallelization*:\n",
    "Het algoritme is geschikt voor parallel leren (bijvoorbeeld op meerdere CPU’s of GPU’s), wat de trainingssnelheid aanzienlijk verhoogt. Onze agent is zodanig opgezet dat deze op een GPU kan draaien.\n",
    "\n",
    "We maken ook gebruik van geheugen. Onze agent houdt tijdens interactie met de omgeving continu bij welke acties, beloningen en observaties hij tegenkomt. Deze worden tijdelijk opgeslagen in een geheugenstructuur en vervolgens gebruikt tijdens het leerproces\n",
    "\n",
    "(DhanushKumar, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44X5UT-Ow-bz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44X5UT-Ow-bz",
    "outputId": "0d1255c4-b817-4f42-ca6c-208a85e2cdc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Warlord/src\n",
      "agents\t   base  env.py\t\t__init__.py   __pycache__   train.py\n",
      "agents.py  env\t google_collab\tppo_agent.py  train_ppo.py  utils.py\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Warlord/src\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Voeg het pad toe van de directory waarin je .py-bestanden staan\n",
    "sys.path.append(os.getcwd())\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Ik_YP6KJJBYE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik_YP6KJJBYE",
    "outputId": "dc82053e-de22-4999-abe3-16049b283068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_0\n"
     ]
    }
   ],
   "source": [
    "env = warlords_v3.env()\n",
    "env.reset()\n",
    "print(env.agent_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaCERGN0Q8wJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaCERGN0Q8wJ",
    "outputId": "ead9a202-528f-4943-af2a-b0ae709a5d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supersuit in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
      "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.1.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install supersuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5kqgzOA-Xaqx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "5kqgzOA-Xaqx",
    "outputId": "5f247e5b-a4f3-4672-8957-7a52f2eb6ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Timestep: 1464/500000, Avg Reward: -0.25\n",
      "\n",
      "--- Updating policy at timestep 2047 ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1712) must match the size of tensor b (1711) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-8-2981056293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Let op: PPO is sneller per update, maar heeft ook veel stappen nodig.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Zorg dat je GPU runtime hebt geselecteerd in Colab.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrained_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_warlords_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/train_ppo.py\u001b[0m in \u001b[0;36mtrain_warlords_ppo\u001b[0;34m(total_timesteps, update_interval)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Updating policy at timestep {timestep} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Print performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/ppo_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# Bereken de surrogate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_to_go\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0msurr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratios\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0msurr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_clip\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1712) must match the size of tensor b (1711) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Warlord/src')\n",
    "\n",
    "# Importeer de nieuwe trainingsfunctie en start de training\n",
    "import train_ppo\n",
    "import importlib\n",
    "importlib.reload(train_ppo)\n",
    "\n",
    "# Start de PPO training!\n",
    "# Let op: PPO is sneller per update, maar heeft ook veel stappen nodig.\n",
    "# Zorg dat je GPU runtime hebt geselecteerd in Colab.\n",
    "trained_agents = train_ppo.train_warlords_ppo(total_timesteps=500_000, update_interval=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uUuLPNkuP7zK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "uUuLPNkuP7zK",
    "outputId": "0be9f2e6-8bd0-482e-d9d7-5a09e3d6b743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode 1/500 | Avg Reward: -0.50 | Epsilon: 0.9995\n",
      "Episode 2/500 | Avg Reward: -0.50 | Epsilon: 0.9990\n",
      "Episode 3/500 | Avg Reward: -0.50 | Epsilon: 0.9985\n",
      "Episode 4/500 | Avg Reward: -0.50 | Epsilon: 0.9980\n",
      "Episode 5/500 | Avg Reward: -0.50 | Epsilon: 0.9975\n",
      "Episode 6/500 | Avg Reward: -0.50 | Epsilon: 0.9970\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-10-33289263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Let op: 5000 episodes kan lang duren. Begin met een kleiner aantal (bv. 500) om te testen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Zorg dat je GPU runtime hebt geselecteerd in Colab voor snellere training (Runtime > Change runtime type > T4 GPU).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrained_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_warlords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dqn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/train.py\u001b[0m in \u001b[0;36mtrain_warlords\u001b[0;34m(n_episodes, use_dqn, learning_rate, target_update_freq, batch_size)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mprev_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/agents.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Warlord/src')\n",
    "\n",
    "# Importeer de trainingsfunctie en start de training\n",
    "# Door 'importlib.reload' te gebruiken, ben je zeker dat je de laatste versie van je script laadt.\n",
    "import train\n",
    "import importlib\n",
    "importlib.reload(train)\n",
    "\n",
    "# Start de training!\n",
    "# Let op: 5000 episodes kan lang duren. Begin met een kleiner aantal (bv. 500) om te testen.\n",
    "# Zorg dat je GPU runtime hebt geselecteerd in Colab voor snellere training (Runtime > Change runtime type > T4 GPU).\n",
    "trained_agents = train.train_warlords(n_episodes=500, use_dqn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_RGCXm2d3a4i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "_RGCXm2d3a4i",
    "outputId": "b8e68ad0-5779-483c-8926-cfac2a3c9ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Reward: 0\n",
      "Episode 1 | Reward: 0\n",
      "Episode 2 | Reward: 0\n",
      "Episode 3 | Reward: 0\n",
      "Episode 4 | Reward: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-8-903996073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_agent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrained_agent_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0magent_dqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0magent_dqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/agents.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#    self.q_net(states_v) geeft (batch_size, n_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#    gather haalt de Q-waarde op voor de gekozen acties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         state_action_values = self.q_net(states_v).gather(\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         ).squeeze(-1)\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/Warlord/src/agents.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAgent1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from env import WarlordsEnv\n",
    "from agents import Agent1, Agent2\n",
    "import numpy as np\n",
    "\n",
    "env = WarlordsEnv()\n",
    "\n",
    "\n",
    "\n",
    "trained_agent_id = env.agent()         # bv. \"green_0\"\n",
    "obs = env.reset()                      # initiële observatie\n",
    "n_actions = env.action_space().n\n",
    "\n",
    "agent_dqn  = Agent1(obs.shape, n_actions)\n",
    "agent_rand = Agent2()\n",
    "\n",
    "EPISODES      = 100\n",
    "BATCH_SIZE    = 32\n",
    "TARGET_UPDATE = 50\n",
    "rewards       = []\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    obs          = env.reset()\n",
    "    current_agent = env.agent()        # wie mag als eerste?\n",
    "    total_reward  = 0\n",
    "    done          = False\n",
    "\n",
    "    while not done:\n",
    "        # Kies actie op basis van de agent die nu speelt\n",
    "        if current_agent == trained_agent_id:\n",
    "            action = agent_dqn.select_action(obs)\n",
    "        else:\n",
    "            action = agent_rand.act(obs)\n",
    "\n",
    "        # Stap vooruit in de env\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_agent = env.agent()       # na de stap, vraag opnieuw wie nu aan zet is\n",
    "\n",
    "        # Leer alleen als het onze DQN-agent was\n",
    "        if current_agent == trained_agent_id:\n",
    "            agent_dqn.replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "            agent_dqn.learn(BATCH_SIZE)\n",
    "            total_reward += reward\n",
    "\n",
    "        # Update voor de volgende iteratie\n",
    "        obs           = next_obs\n",
    "        current_agent = next_agent\n",
    "\n",
    "    # Sync target-netwerk\n",
    "    if ep % TARGET_UPDATE == 0:\n",
    "        agent_dqn.target_net.load_state_dict(agent_dqn.q_net.state_dict())\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {ep} | Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b35fd",
   "metadata": {
    "id": "958b35fd"
   },
   "outputs": [],
   "source": [
    "from utils import ReplayBuffer\n",
    "from agents import Agent1, Agent2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bd15b",
   "metadata": {
    "id": "861bd15b"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 4: Resultaten van het trainen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb04e4",
   "metadata": {
    "id": "49fb04e4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e814f3",
   "metadata": {
    "id": "74e814f3"
   },
   "outputs": [],
   "source": [
    "# --- Laad logs ------------------------------------------\n",
    "log = pd.read_csv(log_dir / \"rewards.csv\")\n",
    "log[\"smooth\"] = log.reward.rolling(50).mean()\n",
    "\n",
    "# --- Plot ------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(log.episode, log.reward, alpha=.3, label=\"Episode reward\")\n",
    "ax.plot(log.episode, log.smooth, lw=2, label=\"Rolling mean (50)\")\n",
    "ax.set_xlabel(\"Episode\"); ax.set_ylabel(\"Reward\")\n",
    "ax.set_title(\"PPO on Warlords – training curve\")\n",
    "ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a320b",
   "metadata": {
    "id": "6b5a320b"
   },
   "outputs": [],
   "source": [
    "# --- Histogram eindscore --------------------------------\n",
    "log.reward.plot.hist(bins=30)\n",
    "plt.title(\"Verdeling episode-reward\")\n",
    "plt.xlabel(\"Reward\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3083af",
   "metadata": {
    "id": "5f3083af"
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "env = make_env(cfg[\"seed\"])\n",
    "agent.load(model_path)          # geladen op correct device\n",
    "\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    act = agent.select_action(obs)\n",
    "    obs, _, done, _ = env.step(act)\n",
    "    frame = env.render(mode=\"rgb_array\")\n",
    "    frames.append(frame)\n",
    "\n",
    "gif_path = log_dir / \"ppo_warlords.gif\"\n",
    "ImageSequenceClip(frames, fps=15).write_gif(gif_path)\n",
    "print(\"GIF opgeslagen:\", gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c84e2-ce0d-4520-abfa-a870f4ba7b39",
   "metadata": {
    "id": "658c84e2-ce0d-4520-abfa-a870f4ba7b39"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 5: Conclusie </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51ec19-6267-4d7e-acbc-0238e32e0178",
   "metadata": {
    "id": "6c51ec19-6267-4d7e-acbc-0238e32e0178"
   },
   "source": [
    "Het is helaas niet gelukt om de agents te kunnen trainen waardoor we nu geen conclusie kunnen schrijven. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d1c40-b682-4ec3-a627-5339357d2e7a",
   "metadata": {
    "id": "7b3d1c40-b682-4ec3-a627-5339357d2e7a"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 6: Bronnenlijst </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21278e-800c-4786-b881-852e305472cd",
   "metadata": {
    "id": "9c21278e-800c-4786-b881-852e305472cd"
   },
   "source": [
    "- DhanushKumar. (2024, 12 mei). PPO Algorithm - DhanushKumar - Medium. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "- Weng, L. (2018, April 8). Policy gradient algorithms. Lil’Log. https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
    "- Proximal Policy Optimization — Spinning Up  documentation. (n.d.). https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "- Proximal Policy Optimization Algorithms. (n.d.). In Arxiv. https://arxiv.org/pdf/1707.06347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "esdfFzlNMBaU",
   "metadata": {
    "id": "esdfFzlNMBaU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
