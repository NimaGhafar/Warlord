{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adedbc2",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:white; text-align:center; padding:20px;\">\n",
    "    <h2 style=\"color:black; font-family: Verdana, sans-serif;\"><strong>Warlord</strong></h2>\n",
    "    <p style=\"font-size: 14px; color: black; font-family: Verdana, sans-serif;\"> \n",
    "        <table style=\"margin: auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 0;\">Names</th>\n",
    "                <th style=\"border: 0;\">GitHub Username</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Vince Ammerlaan</td>\n",
    "                <td style=\"border: 0;\">Vince16270</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Tommi Lander</td>\n",
    "                <td style=\"border: 0;\">tommi7</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Nima Ghafar</td>\n",
    "                <td style=\"border: 0;\">NimaGhafar</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Isa Dijkstra</td>\n",
    "                <td style=\"border: 0;\">IsaD01</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; margin-top: 10px;\">\n",
    "    <img src=\"photo/image1.png\" alt=\" photo needs to be added\" style=\"width: 1000px; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa730d0d",
   "metadata": {},
   "source": [
    "- Docent: Vikram Radhakrishnan\n",
    "- Datum: 20-06-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e751d",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Inhoudsopgave </strong></h2>\n",
    "</div>\n",
    "<ul style=\"padding: 0; list-style: none;\">\n",
    "    <li style=\"text-align: center;\"><a href=\"#1.0\">Chapter 1: Het project</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#2.0\">Chapter 2: Packages en functies inladen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#3.0\">Chapter 3: Trainen van de agents</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#4.0\">Chapter 4: Resultaten van het trainen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 5: Conclusie</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 6: Bronnenlijst</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7f30",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 1: Het project </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c36653",
   "metadata": {},
   "source": [
    "In deze opdracht richten we ons op het computerspel Warlords. In dit spel beschermd elke speler een kasteel dat zich in een van de vier hoeken van het scherm bevindt. Vanuit het midden van het scherm komt een bal die wordt teruggekaatst om de kastelen van de tegenstanders te vernietigen. Elke speler heeft een schild waarmee hij de bal kan afweren of richten op vijanden.\n",
    "\n",
    "Ons doel is om meerdere slimme agents te ontwikkelen die elk zelfstandig leren spelen via multi-agent reinforcement learning. Dat betekent dat elke agent leert op basis van zijn eigen ervaringen, maar ook rekening moet houden met het gedrag van andere agents. De omgeving is dynamisch en strategisch: je moet niet alleen goed verdedigen, maar ook slim aanvallen.\n",
    "\n",
    "Warlords is bij uitstek geschikt voor dit soort leerstrategieën, omdat het meerdere spelers bevat die tegelijk handelen en op elkaar reageren. Dat maakt het leerproces complex en interessant: agents moeten niet alleen leren van hun eigen fouten, maar ook anticiperen op anderen.\n",
    "\n",
    "Het spel eindigt op twee manieren:\n",
    "\n",
    "- Je wint door als laatste overgebleven speler in het spel te zijn.\n",
    "- Je verliest als je kasteel volledig vernietigd is door de bal.\n",
    "\n",
    "**Wat is het probleem?**\n",
    "\n",
    "We willen een slimme agent ontwikkelen die zelfstandig leert om het spel Warlords goed te spelen. Deze agent moet:\n",
    "\n",
    "- Leren hoe de bal beweegt en hoe hij daarop moet reageren\n",
    "- Ontwikkelen hoe je effectief verdedigt én aanvalt\n",
    "- Tactisch aanvallen uitvoeren om de vijandelijke kastelen te verzwakken\n",
    "- Opletten op meerdere tegenstanders tegelijk en zich aanpassen aan hun gedrag\n",
    "\n",
    "**Wat is het doel?**\n",
    "\n",
    "De agent moet:\n",
    "\n",
    "- Zelf leren van hun ervaringen en hun gedrag continu aanpassen\n",
    "- Beter presteren dan agents die willekeurig bewegen of vaste strategieën gebruiken\n",
    "- Verschillende strategieën ontwikkelen afhankelijk van tegenstanders\n",
    "- Leren overleven in een complexe omgeving en uiteindelijk winnen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b48b3",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 2: Packages en functies inladen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4583bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, time, numpy as np, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.env.warlords_env import make_env\n",
    "from src.agents.agent import PPOAgent, RandomAgent\n",
    "from src.utils import device         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6dc9c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0 | device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch:\", torch.__version__, \"| device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602c5bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'ppo': {'max_timesteps': 3000000,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.00025,\n",
       "  'eps_clip': 0.1,\n",
       "  'K_epochs': 4,\n",
       "  'save_path': 'models/ppo_warlords.pt',\n",
       "  'log_interval': 10000}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config laden \n",
    "cfg_path = \"configs/ppo_warlords.yaml\"\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38d3f8",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 3: Trainen van de agents </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca7b1-afbc-4ba0-a032-c2000b9e7371",
   "metadata": {},
   "source": [
    "**PPO Agent: Proximal Policy Optimization**\n",
    "\n",
    "Wij hebben ervoor gekozen om een PPO-agent te trainen. PPO staat voor Proximal Policy Optimization en is een state-of-the-art algoritme dat agents in staat stelt om zelfstandig te leren door middel van trial-and-error in een omgeving.\n",
    "\n",
    "De drie belangrijkste voordelen van PPO zijn:\n",
    "\n",
    "- Eenvoud\n",
    "- Stabiliteit\n",
    "- Steekproefefficiëntie\n",
    "\n",
    "\n",
    "Het PPO-algoritme werkt volgens de volgende principes:\n",
    "\n",
    "- *Policy Gradient Methods*:\n",
    "PPO leert direct welk gedrag (actie) optimaal is in een bepaalde situatie (state), in plaats van alleen in te schatten hoe goed een situatie op zichzelf is.\n",
    "- *Objective Function*:\n",
    "Het algoritme probeert het gedrag van de agent zó aan te passen dat deze op de lange termijn zoveel mogelijk beloning ontvangt uit de omgeving.\n",
    "\n",
    "- *Clipped Surrogate Objective*:\n",
    "PPO voorkomt dat de agent na één leerstap drastisch ander gedrag gaat vertonen. Grote aanpassingen in het gedrag worden ‘geclipt’, waardoor het leerproces stabiel blijft.\n",
    "\n",
    "- *Multiple Epochs & Mini-Batch Updates*:\n",
    "De agent verzamelt ervaringen (observaties, acties, beloningen) in de omgeving. Deze gegevens worden vervolgens herhaaldelijk gebruikt tijdens het leerproces, opgesplitst in kleine, efficiënte mini-batches.\n",
    "\n",
    "- *Value Function Estimation*:\n",
    "PPO maakt ook gebruik van een waarde-inschatting van situaties (de ‘value function’), wat helpt om het leerproces minder schommelend en efficiënter te maken.\n",
    "\n",
    "- *Parallelization*:\n",
    "Het algoritme is geschikt voor parallel leren (bijvoorbeeld op meerdere CPU’s of GPU’s), wat de trainingssnelheid aanzienlijk verhoogt.\n",
    "\n",
    "(DhanushKumar, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cccc71f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ale_py' has no attribute 'list_games'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01male_py\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtextwrap\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m games = [g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[43male_py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_games\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mwarlord\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m g]\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(textwrap.fill(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(games), \u001b[32m80\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# → warlord\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'ale_py' has no attribute 'list_games'"
     ]
    }
   ],
   "source": [
    "import ale_py, textwrap\n",
    "games = [g for g in ale_py.list_games() if \"warlord\" in g]\n",
    "print(textwrap.fill(\" \".join(games), 80))\n",
    "# → warlord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e27a486",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `Warlord` doesn't exist in namespace ALE.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameNotFound\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menv\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwarlords_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m env = \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOK - action space:\u001b[39m\u001b[33m\"\u001b[39m, env.action_space)\n\u001b[32m      4\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School - Autonomous Systems/Warlord/src/env/warlords_env.py:32\u001b[39m, in \u001b[36mmake_env\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_env\u001b[39m(seed: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     22\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[33;03m    gymnasium.Env\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mALE/Warlord-v5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Standaard Atari-preprocessing\u001b[39;00m\n\u001b[32m     35\u001b[39m     env = AtariPreprocessing(\n\u001b[32m     36\u001b[39m         env,\n\u001b[32m     37\u001b[39m         frame_skip=\u001b[32m4\u001b[39m,\n\u001b[32m     38\u001b[39m         grayscale_obs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     39\u001b[39m         scale_obs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     40\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.4/envs/ml-mps/lib/python3.12/site-packages/gymnasium/envs/registration.py:689\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    688\u001b[39m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     env_spec = \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.4/envs/ml-mps/lib/python3.12/site-packages/gymnasium/envs/registration.py:533\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(env_id)\u001b[39m\n\u001b[32m    527\u001b[39m     logger.warn(\n\u001b[32m    528\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    529\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    530\u001b[39m     )\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m    535\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    536\u001b[39m     )\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.4/envs/ml-mps/lib/python3.12/site-packages/gymnasium/envs/registration.py:399\u001b[39m, in \u001b[36m_check_version_exists\u001b[39m\u001b[34m(ns, name, version)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.4/envs/ml-mps/lib/python3.12/site-packages/gymnasium/envs/registration.py:376\u001b[39m, in \u001b[36m_check_name_exists\u001b[39m\u001b[34m(ns, name)\u001b[39m\n\u001b[32m    373\u001b[39m namespace_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m suggestion_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error.NameNotFound(\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[31mNameNotFound\u001b[39m: Environment `Warlord` doesn't exist in namespace ALE."
     ]
    }
   ],
   "source": [
    "from src.env.warlords_env import make_env\n",
    "env = make_env(42)\n",
    "print(\"OK - action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f275c-9be4-4bdb-8d69-0515c0e48756",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(cfg[\"seed\"])\n",
    "\n",
    "obs_dim    = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent      = PPOAgent(obs_dim, action_dim, cfg[\"ppo\"])\n",
    "\n",
    "# Logstructuur\n",
    "log_dir = Path(\"../logs\")\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "reward_log = []\n",
    "loss_log   = []           \n",
    "\n",
    "# --- Train loop -----------------------------------------\n",
    "episodes, timestep = 0, 0\n",
    "obs = env.reset()\n",
    "pbar = tqdm(total=cfg[\"ppo\"][\"max_timesteps\"], desc=\"Timesteps\")\n",
    "\n",
    "while timestep < cfg[\"ppo\"][\"max_timesteps\"]:\n",
    "    action = agent.select_action(obs)\n",
    "    obs, r, done, _ = env.step(action)\n",
    "\n",
    "    agent.store_reward(r, done)\n",
    "    timestep += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    if done:\n",
    "        episodes += 1\n",
    "        reward_log.append({\"episode\": episodes, \"reward\": sum(agent.buffer[\"rewards\"])})\n",
    "        agent.update()                 # ← hier gebeurt PPO-update\n",
    "        obs = env.reset()\n",
    "\n",
    "# Save model & logs\n",
    "model_path = Path(cfg[\"ppo\"][\"save_path\"])\n",
    "model_path.parent.mkdir(exist_ok=True)\n",
    "agent.save(model_path)\n",
    "\n",
    "pd.DataFrame(reward_log).to_csv(log_dir / \"rewards.csv\", index=False)\n",
    "print(f\"Training klaar – {episodes} episodes, model: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bd15b",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 4: Resultaten van het trainen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb04e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e814f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Laad logs ------------------------------------------\n",
    "log = pd.read_csv(log_dir / \"rewards.csv\")\n",
    "log[\"smooth\"] = log.reward.rolling(50).mean()\n",
    "\n",
    "# --- Plot ------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(log.episode, log.reward, alpha=.3, label=\"Episode reward\")\n",
    "ax.plot(log.episode, log.smooth, lw=2, label=\"Rolling mean (50)\")\n",
    "ax.set_xlabel(\"Episode\"); ax.set_ylabel(\"Reward\")\n",
    "ax.set_title(\"PPO on Warlords – training curve\")\n",
    "ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Histogram eindscore --------------------------------\n",
    "log.reward.plot.hist(bins=30)\n",
    "plt.title(\"Verdeling episode-reward\")\n",
    "plt.xlabel(\"Reward\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3083af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "env = make_env(cfg[\"seed\"])\n",
    "agent.load(model_path)          # geladen op correct device\n",
    "\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    act = agent.select_action(obs)\n",
    "    obs, _, done, _ = env.step(act)\n",
    "    frame = env.render(mode=\"rgb_array\")\n",
    "    frames.append(frame)\n",
    "\n",
    "gif_path = log_dir / \"ppo_warlords.gif\"\n",
    "ImageSequenceClip(frames, fps=15).write_gif(gif_path)\n",
    "print(\"GIF opgeslagen:\", gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c84e2-ce0d-4520-abfa-a870f4ba7b39",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 5: Conclusie </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51ec19-6267-4d7e-acbc-0238e32e0178",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b3d1c40-b682-4ec3-a627-5339357d2e7a",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 6: Bronnenlijst </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21278e-800c-4786-b881-852e305472cd",
   "metadata": {},
   "source": [
    "- DhanushKumar. (2024, 12 mei). PPO Algorithm - DhanushKumar - Medium. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "\n",
    "Mogelijke bronnen:\n",
    "\n",
    "- https://arxiv.org/pdf/1707.06347\n",
    "- https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
    "- https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
