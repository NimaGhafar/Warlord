{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adedbc2",
   "metadata": {
    "id": "1adedbc2"
   },
   "source": [
    "\n",
    "<div style=\"background-color:white; text-align:center; padding:20px;\">\n",
    "    <h2 style=\"color:black; font-family: Verdana, sans-serif;\"><strong>Warlord</strong></h2>\n",
    "    <p style=\"font-size: 14px; color: black; font-family: Verdana, sans-serif;\">\n",
    "        <table style=\"margin: auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 0;\">Names</th>\n",
    "                <th style=\"border: 0;\">GitHub Username</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Vince Ammerlaan</td>\n",
    "                <td style=\"border: 0;\">Vince16270</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Tommi Lander</td>\n",
    "                <td style=\"border: 0;\">tommi7</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Nima Ghafar</td>\n",
    "                <td style=\"border: 0;\">NimaGhafar</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"border: 0;\">Isa Dijkstra</td>\n",
    "                <td style=\"border: 0;\">IsaD01</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; margin-top: 10px;\">\n",
    "    <img src=\"photo/image1.png\" alt=\"\" style=\"width: 1000px; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa730d0d",
   "metadata": {
    "id": "fa730d0d"
   },
   "source": [
    "- Docent: Vikram Radhakrishnan\n",
    "- Datum: 22-06-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e751d",
   "metadata": {
    "id": "339e751d"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Inhoudsopgave </strong></h2>\n",
    "</div>\n",
    "<ul style=\"padding: 0; list-style: none;\">\n",
    "    <li style=\"text-align: center;\"><a href=\"#1.0\">Chapter 1: Het project</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#2.0\">Chapter 2: Packages en functies inladen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#3.0\">Chapter 3: Trainen van de agents</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#4.0\">Chapter 4: Resultaten van het trainen</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 5: Conclusie</a></li>\n",
    "    <li style=\"text-align: center;\"><a href=\"#5.0\">Chapter 6: Bronnenlijst</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7f30",
   "metadata": {
    "id": "9ffa7f30"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 1: Het project </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c36653",
   "metadata": {
    "id": "25c36653"
   },
   "source": [
    "In deze opdracht richten we ons op het computerspel Warlords. In dit spel beschermd elke speler een kasteel dat zich in een van de vier hoeken van het scherm bevindt. Vanuit het midden van het scherm komt een bal die wordt teruggekaatst om de kastelen van de tegenstanders te vernietigen. Elke speler heeft een schild waarmee hij de bal kan afweren of richten op vijanden.\n",
    "\n",
    "Ons doel is om een slimme agent te ontwikkelen die zelfstandig leert spelen via multi-agent reinforcement learning. Dat betekent dat de agent leert op basis van zijn eigen ervaringen, maar ook rekening moet houden met het gedrag van andere agents. De omgeving is dynamisch en strategisch: je moet niet alleen goed verdedigen, maar ook slim aanvallen.\n",
    "\n",
    "Warlords is het meest geschikt voor dit soort leerstrategieën, omdat het meerdere spelers bevat die tegelijk handelen en op elkaar reageren. Dat maakt het leerproces complex en interessant: agents moeten niet alleen leren van hun eigen fouten, maar ook anticiperen op anderen.\n",
    "\n",
    "Het spel eindigt op twee manieren:\n",
    "\n",
    "- Je wint door als laatste overgebleven speler in het spel te zijn.\n",
    "- Je verliest als je kasteel volledig vernietigd is door de bal.\n",
    "\n",
    "**Wat is het probleem?**\n",
    "\n",
    "We willen een slimme agent ontwikkelen die zelfstandig leert om het spel Warlords goed te spelen. Deze agent moet:\n",
    "\n",
    "- Leren hoe de bal beweegt en hoe hij daarop moet reageren\n",
    "- Ontwikkelen hoe je effectief verdedigt én aanvalt\n",
    "- Tactisch aanvallen uitvoeren om de vijandelijke kastelen te verzwakken\n",
    "- Opletten op meerdere tegenstanders tegelijk en zich aanpassen aan hun gedrag\n",
    "\n",
    "**Wat is het doel?**\n",
    "\n",
    "De agent moet:\n",
    "\n",
    "- Zelf leren van hun ervaringen en hun gedrag continu aanpassen\n",
    "- Beter presteren dan agents die willekeurig bewegen of vaste strategieën gebruiken\n",
    "- Verschillende strategieën ontwikkelen afhankelijk van tegenstanders\n",
    "- Leren overleven in een complexe omgeving en uiteindelijk winnen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b48b3",
   "metadata": {
    "id": "ed7b48b3"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 2: Packages en functies inladen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583bc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1689,
     "status": "ok",
     "timestamp": 1750619550536,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "4583bc96",
    "outputId": "a31d7872-0ef0-4db4-dfd3-6ebca936ab11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc9c40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14956,
     "status": "ok",
     "timestamp": 1750619567908,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "f6dc9c40",
    "outputId": "ba28caf3-a7d4-49c7-a3d8-73dc4a6c5881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.11/dist-packages (1.25.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.0.2)\n",
      "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (1.1.1)\n",
      "Requirement already satisfied: multi_agent_ale_py>=0.1.11 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (0.1.11)\n",
      "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (0.0.4)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.6.15)\n",
      "Looking in links: dist/\n",
      "Requirement already satisfied: AutoROM[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (8.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2025.6.15)\n",
      "Requirement already satisfied: supersuit in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
      "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.1.1)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.2.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install pettingzoo[atari]\n",
    "!pip install \"autorom[accept-rom-license]\"\n",
    "!pip install --find-links dist/ --no-cache-dir AutoROM[accept-rom-license]\n",
    "!pip install supersuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0hY7YjUvw3mx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23324,
     "status": "ok",
     "timestamp": 1750619591242,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "0hY7YjUvw3mx",
    "outputId": "265a0f78-fd38-4a1a-c39f-dd8b52f9d230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoROM will download the Atari 2600 ROMs.\n",
      "They will be installed to:\n",
      "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
      "\t/usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms\n",
      "\n",
      "Existing ROMs will be overwritten.\n",
      "\n",
      "I own a license to these Atari 2600 ROMs.\n",
      "I agree to not distribute these ROMs and wish to proceed: [Y/n]: Y\n"
     ]
    }
   ],
   "source": [
    "# Start AutoROM\n",
    "\n",
    "!AutoROM\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import supersuit as ss\n",
    "from pettingzoo.atari import warlords_v3\n",
    "from pettingzoo.utils import BaseParallelWrapper\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import importlib\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38d3f8",
   "metadata": {
    "id": "1a38d3f8"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 3: Trainen van de agents - Hyperparameter experiment </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca7b1-afbc-4ba0-a032-c2000b9e7371",
   "metadata": {
    "id": "ec9ca7b1-afbc-4ba0-a032-c2000b9e7371"
   },
   "source": [
    "**PPO Agent: Proximal Policy Optimization**\n",
    "\n",
    "Wij hebben ervoor gekozen om een PPO-agent te trainen. PPO staat voor Proximal Policy Optimization en is een state-of-the-art algoritme dat agents in staat stelt om zelfstandig te leren door middel van trial-and-error in een omgeving.\n",
    "\n",
    "De drie belangrijkste voordelen van PPO zijn:\n",
    "\n",
    "- Eenvoud\n",
    "- Stabiliteit\n",
    "- Steekproefefficiëntie\n",
    "\n",
    "\n",
    "Het PPO-algoritme werkt volgens de volgende principes:\n",
    "\n",
    "- *Policy Gradient Methods*:\n",
    "PPO leert direct welk gedrag (actie) optimaal is in een bepaalde situatie (state), in plaats van alleen in te schatten hoe goed een situatie op zichzelf is. In onze agent is dit terug te zien in hoe acties worden gekozen: op basis van een kansverdeling die voortkomt uit een getraind neuraal netwerk.\n",
    "- *Objective Function*:\n",
    "Het algoritme probeert het gedrag van de agent zo aan te passen dat deze op de lange termijn zoveel mogelijk beloning ontvangt uit de omgeving. Dit gebeurt aan de hand van een speciaal ontworpen loss functie die gebaseerd is op de verwachte toekomstige beloningen. Onze agent optimaliseert deze functie om het gedrag voortdurend te verbeteren.\n",
    "\n",
    "- *Clipped Surrogate Objective*:\n",
    "PPO voorkomt dat de agent na één leerstap drastisch ander gedrag gaat vertonen. Grote aanpassingen in het gedrag worden ‘geclipt’, waardoor het leerproces stabiel blijft. Dit principe is verwerkt in de manier waarop wij de verhouding tussen oude en nieuwe actiekeuzes beperken tijdens het optimaliseren.\n",
    "\n",
    "- *Multiple Epochs & Mini-Batch Updates*:\n",
    "De agent verzamelt ervaringen (observaties, acties, beloningen) in de omgeving. Deze gegevens worden vervolgens herhaaldelijk gebruikt tijdens het leerproces, opgesplitst in kleine, efficiënte mini-batches.\n",
    "\n",
    "- *Value Function Estimation*:\n",
    "PPO maakt ook gebruik van een waarde-inschatting van situaties (de ‘value function’), wat helpt om het leerproces minder schommelend en efficiënter te maken. In onze agent is dit geïntegreerd in het neurale netwerk dat zowel de actievoorspellingen als de waarde-inschattingen doet. Deze waarde-inschatting maakt het leren sneller en stabieler.\n",
    "\n",
    "- *Parallelization*:\n",
    "Het algoritme is geschikt voor parallel leren (bijvoorbeeld op meerdere CPU’s of GPU’s), wat de trainingssnelheid aanzienlijk verhoogt. Onze agent is zodanig opgezet dat deze op een GPU kan draaien.\n",
    "\n",
    "We maken ook gebruik van geheugen. Onze agent houdt tijdens interactie met de omgeving continu bij welke acties, beloningen en observaties hij tegenkomt. Deze worden tijdelijk opgeslagen in een geheugenstructuur en vervolgens gebruikt tijdens het leerproces\n",
    "\n",
    "(DhanushKumar, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44X5UT-Ow-bz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1750626331073,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "44X5UT-Ow-bz",
    "outputId": "af9dc86f-9394-4023-bbc0-65f44129fd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Warlord/src\n",
      "env.py\t       Hyperparam   ppo_agent.py  random_agent.py  train.py\n",
      "google_collab  __init__.py  __pycache__   train_ppo.py\t   utils.py\n",
      "first_0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os, torch, supersuit as ss, imageio\n",
    "from pettingzoo.atari import warlords_v3\n",
    "from random_agent import Agent1\n",
    "import train_ppo\n",
    "from IPython.display import Video, display\n",
    "\n",
    "os.makedirs(\"Hyperparam\", exist_ok=True)\n",
    "\n",
    "%cd /content/drive/MyDrive/Warlord/src\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "!ls\n",
    "\n",
    "env = warlords_v3.env()\n",
    "env.reset()\n",
    "print(env.agent_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "KKzPGeGZb6ke",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750626379058,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "KKzPGeGZb6ke"
   },
   "outputs": [],
   "source": [
    "def make_wrapped_env(render=False):\n",
    "    env = warlords_v3.env(\n",
    "        max_cycles = 10_000,\n",
    "        render_mode = \"rgb_array\" if render else None\n",
    "    )\n",
    "    env = ss.max_observation_v0(env, 2)\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, stack_size=4)\n",
    "    env = ss.dtype_v0(env, dtype=float)\n",
    "    env = ss.normalize_obs_v0(env, env_min=0, env_max=255)\n",
    "    return env\n",
    "\n",
    "def get_action(agent_obj, obs):\n",
    "    if hasattr(agent_obj, \"predict\"):\n",
    "        a, _ = agent_obj.predict(obs, deterministic=True)\n",
    "    elif hasattr(agent_obj, \"choose_action\"):\n",
    "        a, _ = agent_obj.choose_action(obs)\n",
    "    else:\n",
    "        a = agent_obj.act(obs)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c3_v_fvVO",
   "metadata": {
    "id": "b08c3_v_fvVO"
   },
   "source": [
    "**Training van 4 PPO agents met andere hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "xwPMKdCPb6h5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1046622,
     "status": "ok",
     "timestamp": 1750627426587,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "xwPMKdCPb6h5",
    "outputId": "e34fc7f9-c903-4ec1-891c-da86c97ffe26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training exp1 (15000 steps)\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Timestep: 363/15000, Episode 1 finished.\n",
      "Timestep: 517/15000, Episode 2 finished.\n",
      "Timestep: 1426/15000, Episode 3 finished.\n",
      "Timestep: 1748/15000, Episode 4 finished.\n",
      "Timestep: 2602/15000, Episode 5 finished.\n",
      "Timestep: 3455/15000, Episode 6 finished.\n",
      "Timestep: 4381/15000, Episode 7 finished.\n",
      "Timestep: 5084/15000, Episode 8 finished.\n",
      "Timestep: 5749/15000, Episode 9 finished.\n",
      "Timestep: 6415/15000, Episode 10 finished.\n",
      "Timestep: 7092/15000, Episode 11 finished.\n",
      "Timestep: 8078/15000, Episode 12 finished.\n",
      "Timestep: 8828/15000, Episode 13 finished.\n",
      "Timestep: 9267/15000, Episode 14 finished.\n",
      "Timestep: 10675/15000, Episode 15 finished.\n",
      "Timestep: 11495/15000, Episode 16 finished.\n",
      "Timestep: 11955/15000, Episode 17 finished.\n",
      "Timestep: 12066/15000, Episode 18 finished.\n",
      "Timestep: 12501/15000, Episode 19 finished.\n",
      "Timestep: 13233/15000, Episode 20 finished.\n",
      "Timestep: 14204/15000, Episode 21 finished.\n",
      "Timestep: 14504/15000, Episode 22 finished.\n",
      "Training voltooid!\n",
      "\n",
      "Training exp2 (15000 steps)\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Timestep: 111/15000, Episode 1 finished.\n",
      "Timestep: 990/15000, Episode 2 finished.\n",
      "Timestep: 1956/15000, Episode 3 finished.\n",
      "Timestep: 3123/15000, Episode 4 finished.\n",
      "Timestep: 3922/15000, Episode 5 finished.\n",
      "Timestep: 4616/15000, Episode 6 finished.\n",
      "Timestep: 5043/15000, Episode 7 finished.\n",
      "Timestep: 5265/15000, Episode 8 finished.\n",
      "Timestep: 5471/15000, Episode 9 finished.\n",
      "Timestep: 5648/15000, Episode 10 finished.\n",
      "Timestep: 6395/15000, Episode 11 finished.\n",
      "Timestep: 7116/15000, Episode 12 finished.\n",
      "Timestep: 8070/15000, Episode 13 finished.\n",
      "Timestep: 8181/15000, Episode 14 finished.\n",
      "Timestep: 8883/15000, Episode 15 finished.\n",
      "Timestep: 10312/15000, Episode 16 finished.\n",
      "Timestep: 11475/15000, Episode 17 finished.\n",
      "Timestep: 12185/15000, Episode 18 finished.\n",
      "Timestep: 12450/15000, Episode 19 finished.\n",
      "Timestep: 12811/15000, Episode 20 finished.\n",
      "Timestep: 13239/15000, Episode 21 finished.\n",
      "Timestep: 13679/15000, Episode 22 finished.\n",
      "Timestep: 14079/15000, Episode 23 finished.\n",
      "Timestep: 14814/15000, Episode 24 finished.\n",
      "Training voltooid!\n",
      "\n",
      "Training exp3 (15000 steps)\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Timestep: 1064/15000, Episode 1 finished.\n",
      "Timestep: 1986/15000, Episode 2 finished.\n",
      "Timestep: 2229/15000, Episode 3 finished.\n",
      "Timestep: 3086/15000, Episode 4 finished.\n",
      "Timestep: 3407/15000, Episode 5 finished.\n",
      "Timestep: 4826/15000, Episode 6 finished.\n",
      "Timestep: 5245/15000, Episode 7 finished.\n",
      "Timestep: 5405/15000, Episode 8 finished.\n",
      "Timestep: 5531/15000, Episode 9 finished.\n",
      "Timestep: 6170/15000, Episode 10 finished.\n",
      "Timestep: 6660/15000, Episode 11 finished.\n",
      "Timestep: 7147/15000, Episode 12 finished.\n",
      "Timestep: 7258/15000, Episode 13 finished.\n",
      "Timestep: 8078/15000, Episode 14 finished.\n",
      "Timestep: 8234/15000, Episode 15 finished.\n",
      "Timestep: 9203/15000, Episode 16 finished.\n",
      "Timestep: 9835/15000, Episode 17 finished.\n",
      "Timestep: 10418/15000, Episode 18 finished.\n",
      "Timestep: 10761/15000, Episode 19 finished.\n",
      "Timestep: 11095/15000, Episode 20 finished.\n",
      "Timestep: 11184/15000, Episode 21 finished.\n",
      "Timestep: 11293/15000, Episode 22 finished.\n",
      "Timestep: 11434/15000, Episode 23 finished.\n",
      "Timestep: 11986/15000, Episode 24 finished.\n",
      "Timestep: 12227/15000, Episode 25 finished.\n",
      "Timestep: 12804/15000, Episode 26 finished.\n",
      "Timestep: 12944/15000, Episode 27 finished.\n",
      "Timestep: 13238/15000, Episode 28 finished.\n",
      "Timestep: 13349/15000, Episode 29 finished.\n",
      "Timestep: 14253/15000, Episode 30 finished.\n",
      "Timestep: 14861/15000, Episode 31 finished.\n",
      "Training voltooid!\n",
      "\n",
      "Training exp4 (15000 steps)\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Timestep: 902/15000, Episode 1 finished.\n",
      "Timestep: 1547/15000, Episode 2 finished.\n",
      "Timestep: 2570/15000, Episode 3 finished.\n",
      "Timestep: 3931/15000, Episode 4 finished.\n",
      "Timestep: 4626/15000, Episode 5 finished.\n",
      "Timestep: 4787/15000, Episode 6 finished.\n",
      "Timestep: 5712/15000, Episode 7 finished.\n",
      "Timestep: 5881/15000, Episode 8 finished.\n",
      "Timestep: 6660/15000, Episode 9 finished.\n",
      "Timestep: 7100/15000, Episode 10 finished.\n",
      "Timestep: 7280/15000, Episode 11 finished.\n",
      "Timestep: 8078/15000, Episode 12 finished.\n",
      "Timestep: 8278/15000, Episode 13 finished.\n",
      "Timestep: 9101/15000, Episode 14 finished.\n",
      "Timestep: 9245/15000, Episode 15 finished.\n",
      "Timestep: 9948/15000, Episode 16 finished.\n",
      "Timestep: 10952/15000, Episode 17 finished.\n",
      "Timestep: 11651/15000, Episode 18 finished.\n",
      "Timestep: 12725/15000, Episode 19 finished.\n",
      "Timestep: 12850/15000, Episode 20 finished.\n",
      "Timestep: 12961/15000, Episode 21 finished.\n",
      "Timestep: 13821/15000, Episode 22 finished.\n",
      "Timestep: 13910/15000, Episode 23 finished.\n",
      "Training voltooid!\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {\"name\":\"exp1\", \"lr\":3e-4, \"gamma\":0.99, \"eps_clip\":0.20},\n",
    "    {\"name\":\"exp2\", \"lr\":5e-5, \"gamma\":0.99, \"eps_clip\":0.20},\n",
    "    {\"name\":\"exp3\", \"lr\":3e-4, \"gamma\":0.99, \"eps_clip\":0.30},\n",
    "    {\"name\":\"exp4\", \"lr\":3e-4, \"gamma\":0.90, \"eps_clip\":0.20},\n",
    "]\n",
    "\n",
    "TOTAL_STEPS = 15_000\n",
    "UPDATE_INT  = 2_048\n",
    "trained     = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\nTraining {exp['name']} ({TOTAL_STEPS} steps)\")\n",
    "    agents = train_ppo.train_warlords_ppo(\n",
    "        total_timesteps = TOTAL_STEPS,\n",
    "        update_interval = UPDATE_INT,\n",
    "        lr       = exp[\"lr\"],\n",
    "        gamma    = exp[\"gamma\"],\n",
    "        eps_clip = exp[\"eps_clip\"])\n",
    "\n",
    "    trained[exp[\"name\"]] = agents\n",
    "    main_id = list(agents.keys())[0]\n",
    "    torch.save(agents[main_id].policy.state_dict(),\n",
    "               f\"Hyperparam/{exp['name']}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HwKM2i4Wf2Aa",
   "metadata": {
    "id": "HwKM2i4Wf2Aa"
   },
   "source": [
    "**Laat de vier PPO agents tegen elkaar spelen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "nH3wYGhMb6dI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14883,
     "status": "ok",
     "timestamp": 1750627441472,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "nH3wYGhMb6dI",
    "outputId": "584b43a0-4c85-4d51-c343-d3cb8da4dd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemiddelde reward per agent (10 potjes):\n",
      "first_0: 0.00\n",
      "second_0: 0.00\n",
      "third_0: 0.00\n",
      "fourth_0: 0.00\n",
      "\n",
      "Beste policy = first_0  (reward 0.00) opgeslagen: Hyperparam/best_agent.pth\n"
     ]
    }
   ],
   "source": [
    "slot_names = make_wrapped_env().possible_agents\n",
    "policy_map = {}\n",
    "for idx, exp in enumerate(experiments):\n",
    "    pol_id = list(trained[exp[\"name\"]].keys())[0]\n",
    "    policy_map[ slot_names[idx] ] = trained[exp[\"name\"]][pol_id]\n",
    "\n",
    "def play_four_way(agent_map, episodes=10):\n",
    "    env = make_wrapped_env()\n",
    "    totals = {a: 0.0 for a in env.possible_agents}\n",
    "    for _ in range(episodes):\n",
    "        env.reset()\n",
    "        done = {a: False for a in env.agents}\n",
    "        while not all(done.values()):\n",
    "            for agent in env.agent_iter():\n",
    "                obs, rew, term, trunc, _ = env.last()\n",
    "                if term or trunc:\n",
    "                    env.step(None); done[agent] = True; continue\n",
    "                env.step(get_action(agent_map[agent], obs))\n",
    "                totals[agent] += rew\n",
    "    env.close()\n",
    "    return {k: v/episodes for k,v in totals.items()}\n",
    "\n",
    "avg = play_four_way(policy_map, episodes=1)\n",
    "\n",
    "print(\"\\nGemiddelde reward per agent (10 potjes):\")\n",
    "for slot in slot_names:\n",
    "    print(f\"{slot}: {avg[slot]:.2f}\")\n",
    "\n",
    "best_slot = max(avg, key=avg.get)\n",
    "torch.save(policy_map[best_slot].policy.state_dict(), \"Hyperparam/best_agent.pth\")\n",
    "print(f\"\\nBeste policy = {best_slot}  (reward {avg[best_slot]:.2f}) opgeslagen: Hyperparam/best_agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8oerMZhPoWhl",
   "metadata": {
    "id": "8oerMZhPoWhl"
   },
   "source": [
    "## **Chapter 3.1: Agent tegen Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "_wCkcOl-gJGN",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750627700833,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "_wCkcOl-gJGN"
   },
   "outputs": [],
   "source": [
    "import os, torch, supersuit as ss\n",
    "from pettingzoo.atari import warlords_v3\n",
    "from ppo_agent import PPOAgent\n",
    "from random_agent import Agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "n6qNmvdRgKqz",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750627701259,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "n6qNmvdRgKqz"
   },
   "outputs": [],
   "source": [
    "def make_wrapped_env(render=False):\n",
    "    env = warlords_v3.env(\n",
    "        max_cycles = 10_000,\n",
    "        render_mode = \"rgb_array\" if render else None)\n",
    "\n",
    "    env = ss.max_observation_v0(env, 2)\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, stack_size=4)\n",
    "    env = ss.dtype_v0(env, dtype=float)\n",
    "    env = ss.normalize_obs_v0(env, env_min=0, env_max=255)\n",
    "    return env\n",
    "\n",
    "def get_action(agent_obj, obs):\n",
    "    if hasattr(agent_obj, \"predict\"):\n",
    "        a, _ = agent_obj.predict(obs, deterministic=True)\n",
    "    elif hasattr(agent_obj, \"choose_action\"):\n",
    "        a, _ = agent_obj.choose_action(obs)\n",
    "    else:\n",
    "        a = agent_obj.act(obs)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LcQQ1TxqgPmY",
   "metadata": {
    "id": "LcQQ1TxqgPmY"
   },
   "source": [
    "**Laad het beste PPO-model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "vo7CRK5BgM32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1750627704889,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "vo7CRK5BgM32",
    "outputId": "b4e76e65-d718-43c6-8d95-4f8204164a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste PPO-policy geladen: Hyperparam/best_agent.pth\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"Hyperparam/best_agent.pth\"\n",
    "assert os.path.exists(ckpt), \"Train eerst en zorg dat best_agent.pth er is\"\n",
    "\n",
    "tmp_env   = make_wrapped_env()\n",
    "obs_shape = tmp_env.observation_space(\"first_0\").shape\n",
    "action_n  = tmp_env.action_space(\"first_0\").n\n",
    "tmp_env.close()\n",
    "\n",
    "best_policy = PPOAgent(obs_shape, action_n, lr=1e-4)\n",
    "best_policy.policy.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "best_policy.policy.eval()\n",
    "print(\"Beste PPO-policy geladen:\", ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QtL7b_0VgRhV",
   "metadata": {
    "id": "QtL7b_0VgRhV"
   },
   "source": [
    "**Laat het PPO model spelen tegen de random-agent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dPsxSStFgODR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142472,
     "status": "ok",
     "timestamp": 1750627848628,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "dPsxSStFgODR",
    "outputId": "68a6f2b9-da2c-4094-a170-2138a1427778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemiddelde reward vs 3× random (10 potjes):\n",
      "first_0   0.00\n",
      "second_0  0.00\n",
      "third_0   0.00\n",
      "fourth_0  0.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_vs_random(policy, episodes=10):\n",
    "    env     = make_wrapped_env()\n",
    "    totals  = {a: 0.0 for a in env.possible_agents}\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        env.reset()\n",
    "        done = {a: False for a in env.agents}\n",
    "        while not all(done.values()):\n",
    "            for agent in env.agent_iter():\n",
    "                obs, rew, term, trunc, _ = env.last()\n",
    "                if term or trunc:\n",
    "                    env.step(None); done[agent] = True; continue\n",
    "\n",
    "                act = get_action(policy if agent==\"first_0\" else Agent1(), obs)\n",
    "                env.step(act)\n",
    "                totals[agent] += rew\n",
    "    env.close()\n",
    "    return {k: v/episodes for k, v in totals.items()}\n",
    "\n",
    "avg = evaluate_vs_random(best_policy, episodes=10)\n",
    "\n",
    "print(\"\\nGemiddelde reward vs 3× random (10 potjes):\")\n",
    "for agent_name, reward in avg.items():\n",
    "    print(f\"{agent_name:8}  {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wChRuP0agEtF",
   "metadata": {
    "id": "wChRuP0agEtF"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 4: Resultaten van het trainen </strong></h2>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n_4pjFFGikCf",
   "metadata": {
    "id": "n_4pjFFGikCf"
   },
   "source": [
    "**Omdat we geen echte rewards hebben gekregen hebben we ook geen rewards.csv, hierdoor geen visualisaties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "AGXFT4V5d_If",
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1750627621593,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "AGXFT4V5d_If"
   },
   "outputs": [],
   "source": [
    "import os, glob, torch, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from pettingzoo.atari import warlords_v3\n",
    "import supersuit as ss\n",
    "from random_agent import Agent1\n",
    "from ppo_agent import PPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "T9Zds6kqeD5R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 39,
     "status": "error",
     "timestamp": 1750627622367,
     "user": {
      "displayName": "Tommi Lander",
      "userId": "13180588638064987104"
     },
     "user_tz": -120
    },
    "id": "T9Zds6kqeD5R",
    "outputId": "e54fe74c-ae32-49f5-d4f9-c2028aff33ce"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Geen rewards.csv bestand gevonden.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-55-2488938743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlog_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hyperparam/exp*_rewards.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlog_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Geen rewards.csv bestand gevonden.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seaborn-v0_8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Geen rewards.csv bestand gevonden."
     ]
    }
   ],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt, numpy as np, glob, os\n",
    "from pathlib import Path\n",
    "\n",
    "log_files = glob.glob(\"Hyperparam/exp*_rewards.csv\")\n",
    "assert log_files, \"Geen rewards.csv bestand gevonden.\"\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I4eL0EwRfgsV",
   "metadata": {
    "id": "I4eL0EwRfgsV"
   },
   "source": [
    "**Visualiseer de reward curves**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uJTGxla3eFtZ",
   "metadata": {
    "id": "uJTGxla3eFtZ"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "for file in sorted(log_files):\n",
    "    tag = Path(file).stem.split(\"_\")[0]           # 'exp1' etc.\n",
    "    df  = pd.read_csv(file, names=[\"episode\",\"reward\",\"timestep\"])\n",
    "    df[\"smooth\"] = df.reward.rolling(50).mean()\n",
    "\n",
    "    ax.plot(df.episode, df.reward, alpha=.25, label=f\"{tag} raw\")\n",
    "    ax.plot(df.episode, df.smooth, lw=2, label=f\"{tag} smooth\")\n",
    "\n",
    "ax.set_xlabel(\"Episode\"); ax.set_ylabel(\"Reward\")\n",
    "ax.set_title(\"Reward-curves per experiment\")\n",
    "ax.legend(ncol=2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqX8iU8LfjZ1",
   "metadata": {
    "id": "iqX8iU8LfjZ1"
   },
   "source": [
    "**Rolling STD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_LZCsWQ2eHUm",
   "metadata": {
    "id": "_LZCsWQ2eHUm"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,3))\n",
    "for file in sorted(log_files):\n",
    "    tag = Path(file).stem.split(\"_\")[0]\n",
    "    df  = pd.read_csv(file, names=[\"episode\",\"reward\",\"timestep\"])\n",
    "    ax.plot(df.episode, df.reward.rolling(50).std(), label=tag)\n",
    "ax.set_xlabel(\"Episode\"); ax.set_ylabel(\"Rolling σ (50)\")\n",
    "ax.set_title(\"Training-stabiliteit\"); ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8uuHq3YofmWM",
   "metadata": {
    "id": "8uuHq3YofmWM"
   },
   "source": [
    "**Histogram van laatste 100 episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8kblMi7mflCZ",
   "metadata": {
    "id": "8kblMi7mflCZ"
   },
   "outputs": [],
   "source": [
    "LAST = 100\n",
    "plt.figure(figsize=(6,3))\n",
    "for file in sorted(log_files):\n",
    "    tag = Path(file).stem.split(\"_\")[0]\n",
    "    df  = pd.read_csv(file, names=[\"episode\",\"reward\",\"timestep\"])\n",
    "    df.tail(LAST).reward.plot.hist(alpha=.4, bins=25, label=tag)\n",
    "plt.title(f\"Verdeling rewards (laatste {LAST} episodes)\")\n",
    "plt.xlabel(\"Reward\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c84e2-ce0d-4520-abfa-a870f4ba7b39",
   "metadata": {
    "id": "658c84e2-ce0d-4520-abfa-a870f4ba7b39"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 5: Conclusie </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51ec19-6267-4d7e-acbc-0238e32e0178",
   "metadata": {
    "id": "6c51ec19-6267-4d7e-acbc-0238e32e0178"
   },
   "source": [
    "Ondanks meerdere experimenten wat varieerde van 2 000 tot 200 000 trainings­stappen, verschillende learning-rates, γ-waarden en clipranges bleven alle agents zonder beloning. We hebben ook:\n",
    "\n",
    "Potjes verlengd naar 10 000 cycli om ervoor te zorgen dat er blokken gebroken konden worden en Frame-skip-waarden aangepast om het aantal interacties te vergroten.\n",
    "\n",
    "Toch leverden geen van deze instellingen positieve rewards op, waardoor we de prestaties van het PPO-model tegenover de baseline­random agent niet konden beoordelen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d1c40-b682-4ec3-a627-5339357d2e7a",
   "metadata": {
    "id": "7b3d1c40-b682-4ec3-a627-5339357d2e7a"
   },
   "source": [
    "---\n",
    "<div style=\"background-color:white; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
    "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Chapter 6: Bronnenlijst </strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21278e-800c-4786-b881-852e305472cd",
   "metadata": {
    "id": "9c21278e-800c-4786-b881-852e305472cd"
   },
   "source": [
    "- DhanushKumar. (2024, 12 mei). PPO algorithm. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms (arXiv:1707.06347). arXiv. https://arxiv.org/abs/1707.06347\n",
    "\n",
    "- Weng, L. (2018, 8 april). Policy gradient algorithms. Lil’Log. https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
    "\n",
    "- OpenAI. (z.d.). Proximal policy optimization (PPO). Spinning Up in Deep RL. https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
